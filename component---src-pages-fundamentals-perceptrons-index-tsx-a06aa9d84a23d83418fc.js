(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{"7qZE":function(e,t,a){"use strict";a.r(t);a("q1tI");var n=a("soUV"),i=a("vrFN"),o=a("GE6y"),l=a("nuHR");var r="import math\n\nfrom typing import List\n\n\ndef S(y: float) -> float:\n    return 1 / (1 + math.exp(-y))\n\n\nclass Perceptron:\n    def __init__(self, a=0, b=0, alpha=0.05):\n        self.a = a\n        self.b = b\n        self.alpha = alpha\n\n    def prediction(self, x: float) -> float:\n        return S(self.a * x + self.b)\n\n    def loss(self, x: List[float], y: List[float]) -> float:\n        return sum((yi - self.prediction(xi)) ** 2 for xi, yi in zip(x, y))\n\n    def backpropagation(self, x: List[float], y: List[float]):\n        pass  # Implement this.\n\n";var s="import math\n\nfrom typing import List\n\n\ndef S(y: float) -> float:\n    return 1 / (1 + math.exp(-y))\n\n\nclass Perceptron:\n    def __init__(self, a=0, b=0, alpha=0.05):\n        self.a = a\n        self.b = b\n        self.alpha = alpha\n\n    def prediction(self, x: float) -> float:\n        return S(self.a * x + self.b)\n\n    def loss(self, x: List[float], y: List[float]) -> float:\n        return sum((yi - self.prediction(xi)) ** 2 for xi, yi in zip(x, y))\n\n    def backpropagation(self, x: List[float], y: List[float]):\n        Sy = self.prediction(x)\n        dSdy = Sy * (1 - Sy)\n\n        # Compute the gradient.\n        dRSSda = -2 * sum((yi - Sy) * dSdy * xi for xi, yi in zip(x, y))\n        dRSSdb = -2 * sum((yi - Sy) * dSdy for xi, yi in zip(x, y))\n\n        # Update the parameters.\n        self.a -= self.alpha * dRSSda\n        self.b -= self.alpha * dRSSdb\n\n",c=a("Wbzz"),b=a("qKvR");t.default=()=>Object(b.b)(n.a,null,Object(b.b)(i.a,{title:"Perceptrons"}),Object(b.b)("h1",null,"Perceptrons"),Object(b.b)("p",null,"From the previous chapter, we implemented a logistic regression solver using gradient descent. But this isn't enough to solve more complicated regression problems. For example, consider the following data set."),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("p",null,'A logistic regression won\'t be able to model this data set effectively because it contains three separate "regions" of data. Instead, we can model this with the convolution of ',Object(b.b)("em",null,"two")," logistic regressions. The final fit of the function is composed of a logistic regression feeding into the input of the next logistic regression. For now, don't worry too much about the details here. Instead, increase the number of logistic regressions to see how adding more logistic regressions can improve the fit of the data."),Object(b.b)("p",null,'The key idea is that logistic regressions can perform binary classification tasks, but when there are more than two categories, a composition of logistic regressions can better fit the data. Intuitively, each logistic regression that is added adds another "category" that the data can fall into, thus increasing the total number of categories available to be fit.'),Object(b.b)("hr",null),Object(b.b)("p",null,"Before we get too far, let's first encapsulate the behavior of a logistic regression into a single class. This class will be known as a"," ",Object(b.b)("strong",null,"perceptron"),". Implement the function"," ",Object(b.b)("code",null,"backpropagation()"),", which performs a single step of gradient descent. Recall the formulas for the partial derivatives of"," ",Object(b.b)(o.a,{tex:"RSS_{logistic}"})," from the previous chapter,"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\frac{\\partial RSS_{logistic}}{\\partial a} = -2 \\sum_i^n (y_i - S(\\hat y_i)) S(\\hat y_i) (1 - S(\\hat y_i)) x_i"})),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\frac{\\partial RSS_{logistic}}{\\partial b} = -2 \\sum_i^n (y_i - S(\\hat y_i)) S(\\hat y_i) (1 - S(\\hat y_i))"})),Object(b.b)(l.a,{initialContent:r,solution:s,evaluations:[]}),Object(b.b)("p",null,"With a single perceptron, gradient descent can be implemented much more easily."),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)(l.a,{initialContent:r,solution:s,evaluations:[]}),Object(b.b)("p",null,"To extend a single perceptron into multiple layers, consider what happens if one perceptron feeds into another. That is, the output"," ",Object(b.b)(o.a,{tex:"\\hat y"})," of ",Object(b.b)("code",null,"prediction()")," of perceptron 1 becomes the input ",Object(b.b)(o.a,{tex:"x"})," of the next perceptron 2. Conceptually, this looks like"),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)("svg",{xmlns:"http://www.w3.org/2000/svg"},Object(b.b)("defs",null,Object(b.b)("marker",{id:"arrowhead",markerWidth:"10",markerHeight:"7",refX:"0",refY:"3.5",orient:"auto"},Object(b.b)("polygon",{points:"0 0, 10 3.5, 0 7"}))),Object(b.b)("line",{x1:"90",y1:"50",x2:"200",y2:"50",stroke:"#000",markerEnd:"url(#arrowhead)"}),Object(b.b)("circle",{cx:"50",cy:"50",r:"25",stroke:"black",fill:"white"}),Object(b.b)("circle",{cx:"250",cy:"50",r:"25",stroke:"black",fill:"white"}))),Object(b.b)("p",null,"Mathematically, our output ",Object(b.b)(o.a,{tex:"\\hat y"})," becomes"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\hat y = S(a_2 \\times S(a_1 x + b_1) + b_2)"})),Object(b.b)("p",null,"Now, instead of two parameters, ",Object(b.b)(o.a,{tex:"a"})," and"," ",Object(b.b)(o.a,{tex:"b"}),", gradient descent must optimize four parameters,"," ",Object(b.b)(o.a,{tex:"a_1"}),", ",Object(b.b)(o.a,{tex:"b_1"}),", ",Object(b.b)(o.a,{tex:"a_2"}),", and"," ",Object(b.b)(o.a,{tex:"b_2"}),". Luckily, the implementation of a perceptron makes this very easy to do via the ",Object(b.b)("strong",null,"backpropagation algorithm"),". Naming a single step of gradient descent ",Object(b.b)("code",null,"backpropagation()")," ","wasn't just a coincidence!"),Object(b.b)("p",null,"As before, the partial derivatives with respect to each of the four parameters can be computed. Compute and implement the partial derivatives. This step can be difficult, but spend some time manually calculating the derivatives on paper and implementing them. If you need a hint, click the hint tab."),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"RSS = \\sum_i^n (y_i - S(a_2 \\times S(a_1 x_i + b_1) + b_2))^2"})),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("hr",null),Object(b.b)("p",null,"With the computed partial derivatives, a pattern arises between them. If you peeked at the solution, it hints at this approach by defining common variables. Let's visualize this a little better. Given the final prediction"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\hat y = S({\\color{blue} a_2 \\times S({\\color{red} a_1 x_i + b_1}) + b_2})"})),Object(b.b)("p",null,"Define the following variables"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"{\\color{red} \\alpha_i = a_1 x_i + b_1}"})),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"{\\color{blue} \\beta_i = a_2 S(\\alpha_i) + b_2}"})),Object(b.b)("p",null,"and use the following notation"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"S'(x) = \\frac{\\partial S(x)}{\\partial x} = S(x) (1 - S(x))"})),Object(b.b)("p",null,"to simplify the partial derivatives"),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\frac{\\partial RSS}{\\partial a_1} = -2 \\sum_i^n {\\color{green} (y_i - S(\\beta_i)) S'(\\beta_i)} S(\\alpha_i)"})),Object(b.b)("p",{style:{textAlign:"center"}},Object(b.b)(o.a,{tex:"\\frac{\\partial RSS}{\\partial a_2} = -2 \\sum_i^n {\\color{green} (y_i - S(\\beta_i)) S'(\\beta_i)} {\\color{orange} a_1 S'(\\alpha_i)} S(\\beta_i)"})),Object(b.b)("p",null,"From this step, observe that the"," ",Object(b.b)("span",{style:{color:"green"}},"green")," part of the equation remains constant from the first perceptron to the second. This is not a coincidence either, and if we were to consider a third perceptron, the"," ",Object(b.b)("span",{style:{color:"orange"}},"orange")," part of the second perceptron's equation would be constant with the third too."),Object(b.b)("p",null,"We will call this growing portion of the equation the"," ",Object(b.b)("strong",null,"propagation factor"),", which is will propagate backwards through our network. Our backpropagation algorithm will thus become"),Object(b.b)("p",null,Object(b.b)("ol",null,Object(b.b)("li",null,"Take as input the propagation factor from the previous perceptron."),Object(b.b)("li",null,"Compute the gradients for this perceptron based on the propagation factor and the input."),Object(b.b)("li",null,"Return the propagation factor times the weight ",Object(b.b)(o.a,{tex:"a"})," and the derivative ",Object(b.b)(o.a,{tex:"S'(x)"})," to be passed to the next perceptron."))),Object(b.b)("p",null,"Update the previous version of ",Object(b.b)("code",null,"backpropagation()")," to take in a propagation factor as input and return the next propagation factor."),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("hr",null),Object(b.b)("p",null,"With this updated implementation, the perceptron can be used as follows."),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("p",null,"And suddenly, we've created a ",Object(b.b)("strong",null,"multi-layer perceptron"),"! Circling back to the start of this chapter, this two-layer perceptron can be used to classify objects into more categories than a standard logistic regression. Play around with the graph below, add some data points, and relish the fact we wrote this perceptron ",Object(b.b)("em",null,"from scratch"),", not with an external machine learning library."),Object(b.b)("p",null,"PLACEHOLDER"),Object(b.b)("p",null,"This is the end of the fundamentals section of this book. While these chapters covered multi-layer perceptrons with one dimension (",Object(b.b)(o.a,{tex:"a"})," and ",Object(b.b)(o.a,{tex:"b"})," were just floats!) the problem set will have you implement the same logic with vectors and matrices. In the next section, ",Object(b.b)("em",null,"Growing the Network"),", we'll expand beyond a line of perceptrons into full networks of neurons and tackle more complicated prediction problems."),Object(b.b)("div",null,Object(b.b)("ul",null,Object(b.b)("li",null,Object(b.b)(c.Link,{to:"/fundamentals/problem-set/"},"Continue to Problem Set 1")),Object(b.b)("li",null,Object(b.b)(c.Link,{to:"/"},"Go back to the table of contents")))))}}]);
//# sourceMappingURL=component---src-pages-fundamentals-perceptrons-index-tsx-a06aa9d84a23d83418fc.js.map