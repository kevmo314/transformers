{"version":3,"sources":["webpack:///./src/python/fundamentals/regressions/problems/index.ts","webpack:///./src/python/fundamentals/regressions/solutions/index.ts","webpack:///./src/pages/fundamentals/regressions/index.tsx"],"names":["Layout","SEO","title","MathJax","tex","style","width","margin","display","alignItems","justifyContent","type","min","max","value","textAlign","backgroundColor","padding","href","Python","initialContent","problems","solution","solutions","evaluations","to"],"mappings":"uJAKe,M,4NAAA,E,2KAAA,E,kIAAA,E,2OCAA,M,iMAAA,E,2OAAA,E,qRAAA,E,uCCKA,cACb,YAACA,EAAA,EAAD,KACE,YAACC,EAAA,EAAD,CAAKC,MAAM,gBACX,qCACA,iNAGwC,6CAHxC,KAKA,+GAIA,2PAMA,uFACkE,IAChE,YAACC,EAAA,EAAD,CAASC,IAAI,sCAFf,kDAKA,qBACE,qBAAOC,MAAO,CAAEC,MAAO,OAAQC,OAAQ,WACrC,yBACE,sBACE,sBACE,YAACJ,EAAA,EAAD,CAASC,IAAI,SAEf,sBACE,YAACD,EAAA,EAAD,CAASC,IAAI,YARvB,eAeA,qEACgD,IAC9C,YAACD,EAAA,EAAD,CAASC,IAAI,qBAFf,WAGU,YAACD,EAAA,EAAD,CAASC,IAAI,MAHvB,QAGkC,YAACD,EAAA,EAAD,CAASC,IAAI,MAH/C,kGAI6E,IAC3E,YAACD,EAAA,EAAD,CAASC,IAAI,MALf,QAK0B,YAACD,EAAA,EAAD,CAASC,IAAI,MALvC,mCAOA,qBACE,mBACEC,MAAO,CACLG,QAAS,OACTC,WAAY,SACZC,eAAgB,WAGlB,mBAAKL,MAAO,CAAEC,MAAO,SACnB,YAACH,EAAA,EAAD,CAASC,IAAI,OAEf,qBAAOO,KAAK,QAAQC,IAAI,IAAIC,IAAI,MAAMC,MAAM,QAE9C,mBACET,MAAO,CACLG,QAAS,OACTC,WAAY,SACZC,eAAgB,WAGlB,mBAAKL,MAAO,CAAEC,MAAO,SACnB,YAACH,EAAA,EAAD,CAASC,IAAI,OAEf,qBAAOO,KAAK,QAAQC,IAAI,IAAIC,IAAI,MAAMC,MAAM,SAGhD,wDACkC,mCADlC,IAC0D,IACxD,YAACX,EAAA,EAAD,CAASC,IAAI,gBAFf,wDAGS,YAACD,EAAA,EAAD,CAASC,IAAI,QAHtB,sBAGiD,YAACD,EAAA,EAAD,CAASC,IAAI,QAH9D,wCAIkC,YAACD,EAAA,EAAD,CAASC,IAAI,YAJ/C,cAMA,iBAAGC,MAAO,CAAEU,UAAW,WACrB,YAACZ,EAAA,EAAD,CAASC,IAAI,qCAEf,yEACoD,IAClD,qDAFF,8CAKA,iBAAGC,MAAO,CAAEU,UAAW,WACrB,YAACZ,EAAA,EAAD,CAASC,IAAI,qEAEf,8EACyD,IACvD,wCAFF,mCAIA,iBACEC,MAAO,CACLW,gBAAiB,OACjBC,QAAS,SAGX,sPAMA,oCACA,gKAEwE,IACtE,YAACd,EAAA,EAAD,CAASC,IAAI,QAHf,wFAIuC,IACrC,iBAAGc,KAAK,sBAAR,oBALF,gHAOgB,YAACf,EAAA,EAAD,CAASC,IAAI,QAP7B,6FAWF,4EACsD,YAACD,EAAA,EAAD,CAASC,IAAI,MAAO,IAD1E,OAEM,YAACD,EAAA,EAAD,CAASC,IAAI,MAFnB,wFAKA,uBACA,gGAGA,YAACe,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,8FACyE,IACvE,YAACrB,EAAA,EAAD,CAASC,IAAI,MAFf,oNAOA,iBAAGC,MAAO,CAAEU,UAAW,WACrB,YAACZ,EAAA,EAAD,CAASC,IAAI,4EAEf,gEACA,YAACe,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,2GAEU,8CAFV,2DAG6B,+CAH7B,oCAImB,YAACrB,EAAA,EAAD,CAASC,IAAI,MAJhC,QAI2C,YAACD,EAAA,EAAD,CAASC,IAAI,MAJxD,+GAQA,YAACe,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,oDAC8B,+CAD9B,8MAMA,uBACA,8CACA,yEACmD,0CAA0B,IAD7E,QAEO,YAACrB,EAAA,EAAD,CAASC,IAAI,aAFpB,OAEoC,YAACD,EAAA,EAAD,CAASC,IAAI,YAFjD,iHAIsB,+CAJtB,0CAKoB,YAACD,EAAA,EAAD,CAASC,IAAI,MALjC,OAK2C,YAACD,EAAA,EAAD,CAASC,IAAI,MALxD,yGAOoB,YAACD,EAAA,EAAD,CAASC,IAAI,MAPjC,QAO4C,YAACD,EAAA,EAAD,CAASC,IAAI,MAPzD,KASA,2IAIA,oCACA,wLAGoB,+CAHpB,oHAKiB,4CALjB,+FAMsE,IACpE,8CAPF,KASA,iJAEgD,YAACD,EAAA,EAAD,CAASC,IAAI,MAF7D,OAEwE,IACtE,YAACD,EAAA,EAAD,CAASC,IAAI,MAHf,oEAMA,2JAE2D,IACzD,YAACD,EAAA,EAAD,CAASC,IAAI,WAHf,mDAKA,iBAAGC,MAAO,CAAEU,UAAW,WACrB,YAACZ,EAAA,EAAD,CAASC,IAAI,+DAEf,2EACqD,YAACD,EAAA,EAAD,CAASC,IAAI,QAAS,IAD3E,qBAIA,iBAAGC,MAAO,CAAEU,UAAW,WACrB,YAACZ,EAAA,EAAD,CAASC,IAAI,gFAEf,4DACsC,YAACD,EAAA,EAAD,CAASC,IAAI,MADnD,OAC8D,IAC5D,YAACD,EAAA,EAAD,CAASC,IAAI,MAFf,oKAMA,YAACe,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,mhBAUA,kIAIA,uBACE,sBACE,sBACE,YAAC,OAAD,CAAMC,GAAG,8BAAT,4BAEF,sBACE,YAAC,OAAD,CAAMA,GAAG,KAAT","file":"component---src-pages-fundamentals-regressions-index-tsx-4d2ea0b8bbbb81ae470f.js","sourcesContent":["import a from \"./a.py\";\nimport b from \"./b.py\";\nimport c from \"./c.py\";\nimport d from \"./d.py\";\n\nexport default { a, b, c, d };\n","import a from \"./a.py\";\nimport b from \"./b.py\";\nimport c from \"./c.py\";\nimport d from \"./d.py\";\n\nexport default { a, b, c, d };\n","import React from \"react\";\n\nimport Layout from \"../../../components/Layout\";\nimport SEO from \"../../../components/SEO\";\nimport MathJax from \"../../../components/MathJax\";\nimport problems from \"../../../python/fundamentals/regressions/problems\";\nimport solutions from \"../../../python/fundamentals/regressions/solutions\";\nimport Python from \"../../../components/Python\";\nimport { Link } from \"gatsby\";\n\nexport default () => (\n  <Layout>\n    <SEO title=\"Regressions\" />\n    <h1>Regressions</h1>\n    <p>\n      Linear regressions are the foundational building block of neural networks.\n      At its core, linear regressions try to find a line that best represents\n      the points in a dataset, known as the <strong>regression line</strong>.\n    </p>\n    <p>\n      Drag the points on the graph to see how the regression line is influenced\n      by the data.\n    </p>\n    <p>\n      In this chapter, we'll build a solver for linear regressions using\n      gradient descent. While there are analytic solutions for linear\n      regressions, using gradient descent is a primer for problems that do not\n      have analytic answers.\n    </p>\n    <p>\n      Let's start with our data. The data consists of a set of points{\" \"}\n      <MathJax tex=\"\\{(x_1, y_1), ..., (x_n, y_n)\\}\" />. For example, the graph\n      to the right contains\n    </p>\n    <p>\n      <table style={{ width: \"auto\", margin: \"0 auto\" }}>\n        <thead>\n          <tr>\n            <th>\n              <MathJax tex=\"x_i\" />\n            </th>\n            <th>\n              <MathJax tex=\"y_i\" />\n            </th>\n          </tr>\n        </thead>\n      </table>\n      PLACEHOLDER\n    </p>\n    <p>\n      To fit the regression line, it takes the form{\" \"}\n      <MathJax tex=\"\\hat y = ax + b\" />\n      , where <MathJax tex=\"a\" /> and <MathJax tex=\"b\" /> represent the slope\n      and intercept of the line, respectively. Drag the sliders below to see how{\" \"}\n      <MathJax tex=\"a\" /> and <MathJax tex=\"b\" /> influence the regression line.\n    </p>\n    <p>\n      <div\n        style={{\n          display: \"flex\",\n          alignItems: \"center\",\n          justifyContent: \"center\",\n        }}\n      >\n        <div style={{ width: \"24px\" }}>\n          <MathJax tex=\"a\" />\n        </div>\n        <input type=\"range\" min=\"1\" max=\"100\" value=\"50\" />\n      </div>\n      <div\n        style={{\n          display: \"flex\",\n          alignItems: \"center\",\n          justifyContent: \"center\",\n        }}\n      >\n        <div style={{ width: \"24px\" }}>\n          <MathJax tex=\"b\" />\n        </div>\n        <input type=\"range\" min=\"1\" max=\"100\" value=\"50\" />\n      </div>\n    </p>\n    <p>\n      For each point in the data, the <strong>error</strong>,{\" \"}\n      <MathJax tex=\"\\epsilon_i\" />, is determined by the difference between the\n      data's <MathJax tex=\"y_i\" /> value for a given <MathJax tex=\"x_i\" /> and\n      the regression line's predicted <MathJax tex=\"\\hat y\" />. That is,\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\epsilon_i = y_i - (a x_i + b)\" />\n    </p>\n    <p>\n      To compute the total error across all points, the{\" \"}\n      <strong>residual sum of squares</strong> is equal to the sum of the\n      squared error.\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"RSS = \\sum_i^n \\epsilon_i^2 = \\sum_i^n (y_i - (a x_i + b))^2\" />\n    </p>\n    <p>\n      Foreshadowing, a linear regression is a simple form of{\" \"}\n      <strong>prediction</strong> on a two-dimensional data set.\n    </p>\n    <p\n      style={{\n        backgroundColor: \"#EEE\",\n        padding: \"16px\",\n      }}\n    >\n      <div>\n        You may be wondering why the squared error is taken. Directly summing\n        the error won't work, for example consider the following graph, which\n        has a summed error of zero, but the fitted line is not particularly\n        insightful.\n      </div>\n      <p>PLACEHOLDER</p>\n      <div>\n        However, squaring the error is not the only way to mitigate this\n        problem. The absolute value of the error could be taken, for example.{\" \"}\n        <MathJax tex=\"RSS\" /> is chosen due to some numerical properties that\n        make it easy to work with. Recall in{\" \"}\n        <a href=\"/gradient-descent/\">gradient descent</a> that certain learning\n        rates cause the algorithm to not converge. For reasons beyond the scope\n        of this text, <MathJax tex=\"RSS\" /> causes various optimization\n        algorithms to converge much more nicely than other options.\n      </div>\n    </p>\n    <p>\n      In order to optimize the prediction, the parameters <MathJax tex=\"a\" />{\" \"}\n      and <MathJax tex=\"b\" /> are chosen to minimize the total error. Let's\n      implement this with gradient descent.\n    </p>\n    <hr />\n    <p>\n      First, implement the residual sum of squares error for a given dataset.\n    </p>\n    <Python\n      initialContent={problems.a}\n      solution={solutions.a}\n      evaluations={[]}\n    />\n    <p>\n      Next, compute the gradient for the residual sum of squares. Recall the{\" \"}\n      <MathJax tex=\"n\" />\n      -dimensional gradient consists of the partial deriatives in each of the\n      dimensions. Therefore, the gradient for the residual sum of squares for a\n      two-dimensional regression consists of the partial derivatives\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\frac{\\partial RSS}{\\partial a}, \\frac{\\partial RSS}{\\partial b}\" />\n    </p>\n    <p>If you need a hint, click the hint tab.</p>\n    <Python\n      initialContent={problems.b}\n      solution={solutions.b}\n      evaluations={[]}\n    />\n    <p>\n      Now that the gradient is implemented, we'll optimize the two parameters by\n      calling <code>gradient_descent()</code>, which we wrote in the last\n      chapter. Write a function, <code>linear_regression()</code>, that takes in\n      data and returns <MathJax tex=\"a\" /> and <MathJax tex=\"b\" /> that best fit\n      the data. When your code runs, a plot will also appear to visualize the\n      algorithm in action.\n    </p>\n    <Python\n      initialContent={problems.c}\n      solution={solutions.c}\n      evaluations={[]}\n    />\n    <p>\n      With this implementation of <code>linear_regression()</code>, we are now\n      able to infer data and use the function to predict values. In the problem\n      set, we'll dive deeper and explore this idea of predictions, but for now\n      let's continue on to logistic regressions.\n    </p>\n    <hr />\n    <h2>Logistic Regressions</h2>\n    <p>\n      While linear regressions allow the prediction of <em>continuous space</em>{\" \"}\n      from <MathJax tex=\"-\\infty\" /> to <MathJax tex=\"\\infty\" />, the output may\n      not always be in this range. For example, if we wish to build a model of\n      probabilities, or a <strong>binary classifier</strong>, the output is only\n      in the range from <MathJax tex=\"0\" /> to <MathJax tex=\"1\" />. With a\n      standard linear regression, however, the regression line's predictions are\n      not restricted to <MathJax tex=\"0\" /> and <MathJax tex=\"1\" />.\n    </p>\n    <p>\n      Play around with the data to the right and identify points that the linear\n      regression produces invalid output for.\n    </p>\n    <p>PLACEHOLDER</p>\n    <p>\n      To solve this problem, observe that the regression line doesn't have to be\n      linear and can instead be curved. In fact, in our gradient descent\n      implementation of <code>linear_regression()</code>, we had no requirement\n      for the underlying regression line to be linear. A common choice for such\n      a curve is the <strong>logistic curve</strong>. If you have read about\n      neural networks before, this is the most common implementation of a{\" \"}\n      <strong>sigmoid function</strong>.\n    </p>\n    <p>\n      Set the fit to a logistic curve to see how the curve fits the data. With a\n      logistic curve, the output is bounded between <MathJax tex=\"0\" /> and{\" \"}\n      <MathJax tex=\"1\" />, so the regression line will no longer predict invalid\n      outputs.\n    </p>\n    <p>\n      Luckily, the implementation of the logistic curve is reasonably simple. It\n      is computed as a function of the linear regression curve{\" \"}\n      <MathJax tex=\"y=ax+b\" />, specifically by performing the transformation\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"S(y) = \\frac{1}{1 + e^{-y}} = \\frac{1}{1 + e^{-ax - b}}\" />\n    </p>\n    <p>\n      Under this transformation, the logistic regression <MathJax tex=\"RSS\" />{\" \"}\n      error is equal to\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"RSS_{logistic} = \\sum_i^n \\epsilon_i^2 = \\sum_i^n (y_i - S(ax_i + b))^2\" />\n    </p>\n    <p>\n      This error has two free parameters, <MathJax tex=\"a\" /> and{\" \"}\n      <MathJax tex=\"b\" />, that can be optimized via gradient descent. Compute\n      the partial derivatives and implement logistic regression similar to how\n      linear regression is implemented.\n    </p>\n    <Python\n      initialContent={problems.d}\n      solution={solutions.d}\n      evaluations={[]}\n    />\n    <p>\n      In the above problem, the linear regression line is transformed to a\n      logistic curve. You might be wondering why specifically the logistic curve\n      was used to do this mapping. There are, in fact, quite a few sigmoid\n      functions that can also be used for this mapping. Logistic curves have\n      some desirable mathematical properties, but aren't the only curves that\n      can be used. Nevertheless, this text will continue to use them in the\n      spirit of continuing with the bare minimum needed to implement a neural\n      network.\n    </p>\n    <p>\n      Next up, we'll encapsulate logistic regressions into a perceptron, the\n      smallest unit of a neural network.\n    </p>\n    <div>\n      <ul>\n        <li>\n          <Link to=\"/fundamentals/perceptrons/\">Continue to Perceptrons</Link>\n        </li>\n        <li>\n          <Link to=\"/\">Go back to the table of contents</Link>\n        </li>\n      </ul>\n    </div>\n  </Layout>\n);\n"],"sourceRoot":""}