{"version":3,"sources":["webpack:///./src/python/fundamentals/perceptrons/problems/index.tsx","webpack:///./src/python/fundamentals/perceptrons/solutions/index.tsx","webpack:///./src/pages/fundamentals/perceptrons/index.tsx"],"names":["Layout","title","MathJax","tex","style","textAlign","Python","initialContent","problems","solution","solutions","evaluations","xmlns","id","markerWidth","markerHeight","refX","refY","orient","points","x1","y1","x2","y2","stroke","markerEnd","cx","cy","r","fill","color","to"],"mappings":"mKAGe,M,gkBCDA,M,05BCOA,cACb,YAACA,EAAA,EAAD,KACE,YAAC,IAAD,CAAKC,MAAM,gBACX,qCACA,0OAKA,oCACA,yMAGqC,6BAHrC,6UAUA,oYAQA,uBACA,+JAEgE,IAC9D,wCAHF,2BAGsD,IACpD,6CAJF,yGAK8D,IAC5D,YAACC,EAAA,EAAD,CAASC,IAAI,mBANf,+BAQA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,2HAEf,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,uHAEf,YAACG,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,wGAIA,oCACA,YAACL,EAAA,EAAD,CACEC,eAAgBC,EAChBC,SAAUC,EACVC,YAAa,KAEf,2JAE4D,IAC1D,YAACT,EAAA,EAAD,CAASC,IAAI,YAHf,OAG8B,wCAH9B,sCAIoB,YAACD,EAAA,EAAD,CAASC,IAAI,MAJjC,4DAOA,oCACA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,mBAAKO,MAAM,8BACT,wBACE,sBACEC,GAAG,YACHC,YAAY,KACZC,aAAa,IACbC,KAAK,IACLC,KAAK,MACLC,OAAO,QAEP,uBAASC,OAAO,uBAGpB,oBACEC,GAAG,KACHC,GAAG,KACHC,GAAG,MACHC,GAAG,KACHC,OAAO,OACPC,UAAU,oBAEZ,sBAAQC,GAAG,KAAKC,GAAG,KAAKC,EAAE,KAAKJ,OAAO,QAAQK,KAAK,UACnD,sBAAQH,GAAG,MAAMC,GAAG,KAAKC,EAAE,KAAKJ,OAAO,QAAQK,KAAK,YAGxD,mDAC6B,YAAC3B,EAAA,EAAD,CAASC,IAAI,YAD1C,YAGA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,mDAEf,wDACkC,YAACD,EAAA,EAAD,CAASC,IAAI,MAD/C,OAC0D,IACxD,YAACD,EAAA,EAAD,CAASC,IAAI,MAFf,oDAEuE,IACrE,YAACD,EAAA,EAAD,CAASC,IAAI,QAHf,KAGyB,YAACD,EAAA,EAAD,CAASC,IAAI,QAHtC,KAGgD,YAACD,EAAA,EAAD,CAASC,IAAI,QAH7D,QAG2E,IACzE,YAACD,EAAA,EAAD,CAASC,IAAI,QAJf,oFAK+B,uDAL/B,8CAM2C,6CAA+B,IAN1E,8BASA,oUAOA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,qEAEf,oCACA,uBACA,sPAMA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,oFAEf,uDACA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,8CAEf,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,uDAEf,uDACA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,mEAEf,4DACA,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,wHAEf,iBAAGC,MAAO,CAAEC,UAAW,WACrB,YAACH,EAAA,EAAD,CAASC,IAAI,2JAEf,wDACmC,IACjC,oBAAMC,MAAO,CAAE0B,MAAO,UAAtB,SAFF,uKAIyE,IACvE,oBAAM1B,MAAO,CAAE0B,MAAO,WAAtB,UALF,mFAQA,6EACwD,IACtD,gDAFF,2GAKA,qBACE,sBACE,4FAGA,kHAIA,wEACiD,YAAC5B,EAAA,EAAD,CAASC,IAAI,MAD9D,uBAEiB,YAACD,EAAA,EAAD,CAASC,IAAI,UAF9B,2CAOJ,uDACiC,6CADjC,qFAIA,oCACA,uBACA,iGAGA,oCACA,sDACgC,oDADhC,0QAK2C,sCAL3C,oDAQA,oCACA,2JAGE,YAACD,EAAA,EAAD,CAASC,IAAI,MAHf,QAG0B,YAACD,EAAA,EAAD,CAASC,IAAI,MAHvC,8HAKoB,6CALpB,8HASA,uBACE,sBACE,sBACE,YAAC,OAAD,CAAM4B,GAAG,8BAAT,8BAEF,sBACE,YAAC,OAAD,CAAMA,GAAG,KAAT","file":"component---src-pages-fundamentals-perceptrons-index-tsx-a06aa9d84a23d83418fc.js","sourcesContent":["import * as a from \"./a.py\";\nimport * as b from \"./b.py\";\n\nexport default { a, b };\n","import * as a from \"./a.py\";\n\nexport default { a };\n","import React from \"react\";\nimport Layout from \"../../../components/Layout\";\nimport SEO from \"../../../components/seo\";\nimport MathJax from \"../../../components/MathJax\";\nimport Python from \"../../../components/Python\";\nimport problems from \"../../../python/fundamentals/perceptrons/problems\";\nimport solutions from \"../../../python/fundamentals/perceptrons/solutions\";\nimport { Link } from \"gatsby\";\n\nexport default () => (\n  <Layout>\n    <SEO title=\"Perceptrons\" />\n    <h1>Perceptrons</h1>\n    <p>\n      From the previous chapter, we implemented a logistic regression solver\n      using gradient descent. But this isn't enough to solve more complicated\n      regression problems. For example, consider the following data set.\n    </p>\n    <p>PLACEHOLDER</p>\n    <p>\n      A logistic regression won't be able to model this data set effectively\n      because it contains three separate \"regions\" of data. Instead, we can\n      model this with the convolution of <em>two</em> logistic regressions. The\n      final fit of the function is composed of a logistic regression feeding\n      into the input of the next logistic regression. For now, don't worry too\n      much about the details here. Instead, increase the number of logistic\n      regressions to see how adding more logistic regressions can improve the\n      fit of the data.\n    </p>\n    <p>\n      The key idea is that logistic regressions can perform binary\n      classification tasks, but when there are more than two categories, a\n      composition of logistic regressions can better fit the data. Intuitively,\n      each logistic regression that is added adds another \"category\" that the\n      data can fall into, thus increasing the total number of categories\n      available to be fit.\n    </p>\n    <hr />\n    <p>\n      Before we get too far, let's first encapsulate the behavior of a logistic\n      regression into a single class. This class will be known as a{\" \"}\n      <strong>perceptron</strong>. Implement the function{\" \"}\n      <code>backpropagation()</code>, which performs a single step of gradient\n      descent. Recall the formulas for the partial derivatives of{\" \"}\n      <MathJax tex=\"RSS_{logistic}\" /> from the previous chapter,\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\frac{\\partial RSS_{logistic}}{\\partial a} = -2 \\sum_i^n (y_i - S(\\hat y_i)) S(\\hat y_i) (1 - S(\\hat y_i)) x_i\" />\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\frac{\\partial RSS_{logistic}}{\\partial b} = -2 \\sum_i^n (y_i - S(\\hat y_i)) S(\\hat y_i) (1 - S(\\hat y_i))\" />\n    </p>\n    <Python\n      initialContent={problems.a}\n      solution={solutions.a}\n      evaluations={[]}\n    />\n    <p>\n      With a single perceptron, gradient descent can be implemented much more\n      easily.\n    </p>\n    <p>PLACEHOLDER</p>\n    <Python\n      initialContent={problems.a}\n      solution={solutions.a}\n      evaluations={[]}\n    />\n    <p>\n      To extend a single perceptron into multiple layers, consider what happens\n      if one perceptron feeds into another. That is, the output{\" \"}\n      <MathJax tex=\"\\hat y\" /> of <code>prediction()</code> of perceptron 1\n      becomes the input <MathJax tex=\"x\" /> of the next perceptron 2.\n      Conceptually, this looks like\n    </p>\n    <p>PLACEHOLDER</p>\n    <p style={{ textAlign: \"center\" }}>\n      <svg xmlns=\"http://www.w3.org/2000/svg\">\n        <defs>\n          <marker\n            id=\"arrowhead\"\n            markerWidth=\"10\"\n            markerHeight=\"7\"\n            refX=\"0\"\n            refY=\"3.5\"\n            orient=\"auto\"\n          >\n            <polygon points=\"0 0, 10 3.5, 0 7\" />\n          </marker>\n        </defs>\n        <line\n          x1=\"90\"\n          y1=\"50\"\n          x2=\"200\"\n          y2=\"50\"\n          stroke=\"#000\"\n          markerEnd=\"url(#arrowhead)\"\n        />\n        <circle cx=\"50\" cy=\"50\" r=\"25\" stroke=\"black\" fill=\"white\" />\n        <circle cx=\"250\" cy=\"50\" r=\"25\" stroke=\"black\" fill=\"white\" />\n      </svg>\n    </p>\n    <p>\n      Mathematically, our output <MathJax tex=\"\\hat y\" /> becomes\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\hat y = S(a_2 \\times S(a_1 x + b_1) + b_2)\" />\n    </p>\n    <p>\n      Now, instead of two parameters, <MathJax tex=\"a\" /> and{\" \"}\n      <MathJax tex=\"b\" />, gradient descent must optimize four parameters,{\" \"}\n      <MathJax tex=\"a_1\" />, <MathJax tex=\"b_1\" />, <MathJax tex=\"a_2\" />, and{\" \"}\n      <MathJax tex=\"b_2\" />. Luckily, the implementation of a perceptron makes\n      this very easy to do via the <strong>backpropagation algorithm</strong>.\n      Naming a single step of gradient descent <code>backpropagation()</code>{\" \"}\n      wasn't just a coincidence!\n    </p>\n    <p>\n      As before, the partial derivatives with respect to each of the four\n      parameters can be computed. Compute and implement the partial derivatives.\n      This step can be difficult, but spend some time manually calculating the\n      derivatives on paper and implementing them. If you need a hint, click the\n      hint tab.\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"RSS = \\sum_i^n (y_i - S(a_2 \\times S(a_1 x_i + b_1) + b_2))^2\" />\n    </p>\n    <p>PLACEHOLDER</p>\n    <hr />\n    <p>\n      With the computed partial derivatives, a pattern arises between them. If\n      you peeked at the solution, it hints at this approach by defining common\n      variables. Let's visualize this a little better. Given the final\n      prediction\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\hat y = S({\\color{blue} a_2 \\times S({\\color{red} a_1 x_i + b_1}) + b_2})\" />\n    </p>\n    <p>Define the following variables</p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"{\\color{red} \\alpha_i = a_1 x_i + b_1}\" />\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"{\\color{blue} \\beta_i = a_2 S(\\alpha_i) + b_2}\" />\n    </p>\n    <p>and use the following notation</p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"S'(x) = \\frac{\\partial S(x)}{\\partial x} = S(x) (1 - S(x))\" />\n    </p>\n    <p>to simplify the partial derivatives</p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\frac{\\partial RSS}{\\partial a_1} = -2 \\sum_i^n {\\color{green} (y_i - S(\\beta_i)) S'(\\beta_i)} S(\\alpha_i)\" />\n    </p>\n    <p style={{ textAlign: \"center\" }}>\n      <MathJax tex=\"\\frac{\\partial RSS}{\\partial a_2} = -2 \\sum_i^n {\\color{green} (y_i - S(\\beta_i)) S'(\\beta_i)} {\\color{orange} a_1 S'(\\alpha_i)} S(\\beta_i)\" />\n    </p>\n    <p>\n      From this step, observe that the{\" \"}\n      <span style={{ color: \"green\" }}>green</span> part of the equation remains\n      constant from the first perceptron to the second. This is not a\n      coincidence either, and if we were to consider a third perceptron, the{\" \"}\n      <span style={{ color: \"orange\" }}>orange</span> part of the second\n      perceptron's equation would be constant with the third too.\n    </p>\n    <p>\n      We will call this growing portion of the equation the{\" \"}\n      <strong>propagation factor</strong>, which is will propagate backwards\n      through our network. Our backpropagation algorithm will thus become\n    </p>\n    <p>\n      <ol>\n        <li>\n          Take as input the propagation factor from the previous perceptron.\n        </li>\n        <li>\n          Compute the gradients for this perceptron based on the propagation\n          factor and the input.\n        </li>\n        <li>\n          Return the propagation factor times the weight <MathJax tex=\"a\" /> and\n          the derivative <MathJax tex=\"S'(x)\" /> to be passed to the next\n          perceptron.\n        </li>\n      </ol>\n    </p>\n    <p>\n      Update the previous version of <code>backpropagation()</code> to take in a\n      propagation factor as input and return the next propagation factor.\n    </p>\n    <p>PLACEHOLDER</p>\n    <hr />\n    <p>\n      With this updated implementation, the perceptron can be used as follows.\n    </p>\n    <p>PLACEHOLDER</p>\n    <p>\n      And suddenly, we've created a <strong>multi-layer perceptron</strong>!\n      Circling back to the start of this chapter, this two-layer perceptron can\n      be used to classify objects into more categories than a standard logistic\n      regression. Play around with the graph below, add some data points, and\n      relish the fact we wrote this perceptron <em>from scratch</em>, not with\n      an external machine learning library.\n    </p>\n    <p>PLACEHOLDER</p>\n    <p>\n      This is the end of the fundamentals section of this book. While these\n      chapters covered multi-layer perceptrons with one dimension (\n      <MathJax tex=\"a\" /> and <MathJax tex=\"b\" /> were just floats!) the problem\n      set will have you implement the same logic with vectors and matrices. In\n      the next section, <em>Growing the Network</em>, we'll expand beyond a line\n      of perceptrons into full networks of neurons and tackle more complicated\n      prediction problems.\n    </p>\n    <div>\n      <ul>\n        <li>\n          <Link to=\"/fundamentals/problem-set/\">Continue to Problem Set 1</Link>\n        </li>\n        <li>\n          <Link to=\"/\">Go back to the table of contents</Link>\n        </li>\n      </ul>\n    </div>\n  </Layout>\n);\n"],"sourceRoot":""}