(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{"9+d2":function(e,t,n){"use strict";n.r(t);n("q1tI");var a=n("soUV"),i=n("vrFN"),o=n("GE6y");var r="import numpy\n\ndata: List[Tuple[float, float]],\n\ndef rss(a: float, b: float) -> float:\n    # data is of form [[x1, y1], [x2, y2], ..., [xn, yn]]\n    x_i = data[:, 0]\n    y_i = data[:, 1]\n    # Implement this.\n",l="import numpy\n\ndata: numpy.ndarray\n\n\ndef grad_rss_a(a: float, b: float) -> Tuple[float, float]:\n    y_i = data[:, 1]\n    x_i = data[:, 0]\n    # Implement this.\n",s="import numpy\n\n\ndef linear_regression(data: List[Tuple[float, float]]) -> Tuple[float, float]:\n    pass  # Implement this.\n",b="def S(y: float) -> float:\n    pass  # Implement this.\n\n\ndef grad_S_a(a: float, x: float, b: float) -> float:\n    pass  # Implement this.\n\n\ndef grad_S_a(a: float, x: float, b: float) -> float:\n    pass  # Implement this.\n\n";var c="from typing import List, Tuple\n\n\ndef rss(data: numpy.ndarray, a: float, b: float) -> float:\n    x_i = data[:, 0]\n    y_i = data[:, 1]\n    return numpy.sum((y_i - (a * x_i + b)) ** 2)\n",u="import numpy\n\n\ndef grad_rss(a: float, b: float) -> float:\n    epsilon_i = y_i - (a * x_i + b)\n\n    partial_a = -2 * numpy.sum(x_i * epsilon_i)\n    partial_b = -2 * numpy.sum(epsilon_i)\n\n    return (partial_a, partial_b)\n\n",h="def linear_regression(data: List[Tuple[float, float]]) -> Tuple[float, float]:\n    return gradient_descent_2d(\n        grad_x=lambda a, b: grad_rss_a(data, a, b),\n        grad_y=lambda a, b: grad_rss_b(data, a, b),\n        alpha=0.1,\n        a=0,\n        b=0,\n    )\n",d="",p=n("nuHR"),m=n("Wbzz"),f=n("qKvR");t.default=()=>Object(f.b)(a.a,null,Object(f.b)(i.a,{title:"Regressions"}),Object(f.b)("h1",null,"Regressions"),Object(f.b)("p",null,"Linear regressions are the foundational building block of neural networks. At its core, linear regressions try to find a line that best represents the points in a dataset, known as the ",Object(f.b)("strong",null,"regression line"),"."),Object(f.b)("p",null,"Drag the points on the graph to see how the regression line is influenced by the data."),Object(f.b)("p",null,"In this chapter, we'll build a solver for linear regressions using gradient descent. While there are analytic solutions for linear regressions, using gradient descent is a primer for problems that do not have analytic answers."),Object(f.b)("p",null,"Let's start with our data. The data consists of a set of points"," ",Object(f.b)(o.a,{tex:"\\{(x_1, y_1), ..., (x_n, y_n)\\}"}),". For example, the graph to the right contains"),Object(f.b)("p",null,Object(f.b)("table",{style:{width:"auto",margin:"0 auto"}},Object(f.b)("thead",null,Object(f.b)("tr",null,Object(f.b)("th",null,Object(f.b)(o.a,{tex:"x_i"})),Object(f.b)("th",null,Object(f.b)(o.a,{tex:"y_i"}))))),"PLACEHOLDER"),Object(f.b)("p",null,"To fit the regression line, it takes the form"," ",Object(f.b)(o.a,{tex:"\\hat y = ax + b"}),", where ",Object(f.b)(o.a,{tex:"a"})," and ",Object(f.b)(o.a,{tex:"b"})," represent the slope and intercept of the line, respectively. Drag the sliders below to see how"," ",Object(f.b)(o.a,{tex:"a"})," and ",Object(f.b)(o.a,{tex:"b"})," influence the regression line."),Object(f.b)("p",null,Object(f.b)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"}},Object(f.b)("div",{style:{width:"24px"}},Object(f.b)(o.a,{tex:"a"})),Object(f.b)("input",{type:"range",min:"1",max:"100",value:"50"})),Object(f.b)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"}},Object(f.b)("div",{style:{width:"24px"}},Object(f.b)(o.a,{tex:"b"})),Object(f.b)("input",{type:"range",min:"1",max:"100",value:"50"}))),Object(f.b)("p",null,"For each point in the data, the ",Object(f.b)("strong",null,"error"),","," ",Object(f.b)(o.a,{tex:"\\epsilon_i"}),", is determined by the difference between the data's ",Object(f.b)(o.a,{tex:"y_i"})," value for a given ",Object(f.b)(o.a,{tex:"x_i"})," and the regression line's predicted ",Object(f.b)(o.a,{tex:"\\hat y"}),". That is,"),Object(f.b)("p",{style:{textAlign:"center"}},Object(f.b)(o.a,{tex:"\\epsilon_i = y_i - (a x_i + b)"})),Object(f.b)("p",null,"To compute the total error across all points, the"," ",Object(f.b)("strong",null,"residual sum of squares")," is equal to the sum of the squared error."),Object(f.b)("p",{style:{textAlign:"center"}},Object(f.b)(o.a,{tex:"RSS = \\sum_i^n \\epsilon_i^2 = \\sum_i^n (y_i - (a x_i + b))^2"})),Object(f.b)("p",null,"Foreshadowing, a linear regression is a simple form of"," ",Object(f.b)("strong",null,"prediction")," on a two-dimensional data set."),Object(f.b)("p",{style:{backgroundColor:"#EEE",padding:"16px"}},Object(f.b)("div",null,"You may be wondering why the squared error is taken. Directly summing the error won't work, for example consider the following graph, which has a summed error of zero, but the fitted line is not particularly insightful."),Object(f.b)("p",null,"PLACEHOLDER"),Object(f.b)("div",null,"However, squaring the error is not the only way to mitigate this problem. The absolute value of the error could be taken, for example."," ",Object(f.b)(o.a,{tex:"RSS"})," is chosen due to some numerical properties that make it easy to work with. Recall in"," ",Object(f.b)("a",{href:"/gradient-descent/"},"gradient descent")," that certain learning rates cause the algorithm to not converge. For reasons beyond the scope of this text, ",Object(f.b)(o.a,{tex:"RSS"})," causes various optimization algorithms to converge much more nicely than other options.")),Object(f.b)("p",null,"In order to optimize the prediction, the parameters ",Object(f.b)(o.a,{tex:"a"})," ","and ",Object(f.b)(o.a,{tex:"b"})," are chosen to minimize the total error. Let's implement this with gradient descent."),Object(f.b)("hr",null),Object(f.b)("p",null,"First, implement the residual sum of squares error for a given dataset."),Object(f.b)(p.a,{initialContent:r,solution:c,evaluations:[]}),Object(f.b)("p",null,"Next, compute the gradient for the residual sum of squares. Recall the"," ",Object(f.b)(o.a,{tex:"n"}),"-dimensional gradient consists of the partial deriatives in each of the dimensions. Therefore, the gradient for the residual sum of squares for a two-dimensional regression consists of the partial derivatives"),Object(f.b)("p",{style:{textAlign:"center"}},Object(f.b)(o.a,{tex:"\\frac{\\partial RSS}{\\partial a}, \\frac{\\partial RSS}{\\partial b}"})),Object(f.b)("p",null,"If you need a hint, click the hint tab."),Object(f.b)(p.a,{initialContent:l,solution:u,evaluations:[]}),Object(f.b)("p",null,"Now that the gradient is implemented, we'll optimize the two parameters by calling ",Object(f.b)("code",null,"gradient_descent()"),", which we wrote in the last chapter. Write a function, ",Object(f.b)("code",null,"linear_regression()"),", that takes in data and returns ",Object(f.b)(o.a,{tex:"a"})," and ",Object(f.b)(o.a,{tex:"b"})," that best fit the data. When your code runs, a plot will also appear to visualize the algorithm in action."),Object(f.b)(p.a,{initialContent:s,solution:h,evaluations:[]}),Object(f.b)("p",null,"With this implementation of ",Object(f.b)("code",null,"linear_regression()"),", we are now able to infer data and use the function to predict values. In the problem set, we'll dive deeper and explore this idea of predictions, but for now let's continue on to logistic regressions."),Object(f.b)("hr",null),Object(f.b)("h2",null,"Logistic Regressions"),Object(f.b)("p",null,"While linear regressions allow the prediction of ",Object(f.b)("em",null,"continuous space")," ","from ",Object(f.b)(o.a,{tex:"-\\infty"})," to ",Object(f.b)(o.a,{tex:"\\infty"}),", the output may not always be in this range. For example, if we wish to build a model of probabilities, or a ",Object(f.b)("strong",null,"binary classifier"),", the output is only in the range from ",Object(f.b)(o.a,{tex:"0"})," to ",Object(f.b)(o.a,{tex:"1"}),". With a standard linear regression, however, the regression line's predictions are not restricted to ",Object(f.b)(o.a,{tex:"0"})," and ",Object(f.b)(o.a,{tex:"1"}),"."),Object(f.b)("p",null,"Play around with the data to the right and identify points that the linear regression produces invalid output for."),Object(f.b)("p",null,"PLACEHOLDER"),Object(f.b)("p",null,"To solve this problem, observe that the regression line doesn't have to be linear and can instead be curved. In fact, in our gradient descent implementation of ",Object(f.b)("code",null,"linear_regression()"),", we had no requirement for the underlying regression line to be linear. A common choice for such a curve is the ",Object(f.b)("strong",null,"logistic curve"),". If you have read about neural networks before, this is the most common implementation of a"," ",Object(f.b)("strong",null,"sigmoid function"),"."),Object(f.b)("p",null,"Set the fit to a logistic curve to see how the curve fits the data. With a logistic curve, the output is bounded between ",Object(f.b)(o.a,{tex:"0"})," and"," ",Object(f.b)(o.a,{tex:"1"}),", so the regression line will no longer predict invalid outputs."),Object(f.b)("p",null,"Luckily, the implementation of the logistic curve is reasonably simple. It is computed as a function of the linear regression curve"," ",Object(f.b)(o.a,{tex:"y=ax+b"}),", specifically by performing the transformation"),Object(f.b)("p",{style:{textAlign:"center"}},Object(f.b)(o.a,{tex:"S(y) = \\frac{1}{1 + e^{-y}} = \\frac{1}{1 + e^{-ax - b}}"})),Object(f.b)("p",null,"Under this transformation, the logistic regression ",Object(f.b)(o.a,{tex:"RSS"})," ","error is equal to"),Object(f.b)("p",{style:{textAlign:"center"}},Object(f.b)(o.a,{tex:"RSS_{logistic} = \\sum_i^n \\epsilon_i^2 = \\sum_i^n (y_i - S(ax_i + b))^2"})),Object(f.b)("p",null,"This error has two free parameters, ",Object(f.b)(o.a,{tex:"a"})," and"," ",Object(f.b)(o.a,{tex:"b"}),", that can be optimized via gradient descent. Compute the partial derivatives and implement logistic regression similar to how linear regression is implemented."),Object(f.b)(p.a,{initialContent:b,solution:d,evaluations:[]}),Object(f.b)("p",null,"In the above problem, the linear regression line is transformed to a logistic curve. You might be wondering why specifically the logistic curve was used to do this mapping. There are, in fact, quite a few sigmoid functions that can also be used for this mapping. Logistic curves have some desirable mathematical properties, but aren't the only curves that can be used. Nevertheless, this text will continue to use them in the spirit of continuing with the bare minimum needed to implement a neural network."),Object(f.b)("p",null,"Next up, we'll encapsulate logistic regressions into a perceptron, the smallest unit of a neural network."),Object(f.b)("div",null,Object(f.b)("ul",null,Object(f.b)("li",null,Object(f.b)(m.Link,{to:"/fundamentals/perceptrons/"},"Continue to Perceptrons")),Object(f.b)("li",null,Object(f.b)(m.Link,{to:"/"},"Go back to the table of contents")))))}}]);
//# sourceMappingURL=component---src-pages-fundamentals-regressions-index-tsx-dd738b22518e4eb814bc.js.map